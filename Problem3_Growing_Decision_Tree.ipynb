{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §0 Loading basic modules, data, demonstrating information gain computations\n",
    "\n",
    "You can safely skip to [§1](#§1-Growing-Decision-Tree) where we proceed to step through growing the tree. Here I am just importing the data, giving example uses of my information gain related methods, and initializing the DTL pipeline I will use in §1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.DTL_methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 observations consisting of 10 features, including target\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9\n",
       "0  1  0  0  1  2  0  1  0  0  1\n",
       "1  1  0  0  2  0  0  0  1  2  0\n",
       "2  0  1  0  1  0  0  0  2  0  1\n",
       "3  1  0  1  2  0  1  0  1  1  1\n",
       "4  1  0  1  2  2  0  1  0  3  0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(): \n",
    "    \"\"\"\n",
    "    10 features with 12 observations\n",
    "    hard coded from problem statement\n",
    "    loaded in with features as rows, then transformed with features as columns\n",
    "    target is last column\n",
    "    \"\"\"\n",
    "\n",
    "    ### LOAD DATA\n",
    "    Alt = [ \"YES\", \"YES\", \"NO\", \"YES\", \"YES\", \"NO\", \"NO\", \"NO\", \"NO\", \"YES\", \"NO\", \"YES\" ]\n",
    "    Bar = [\"NO\", \"NO\", \"YES\", \"NO\", \"NO\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\", \"NO\", \"YES\"]\n",
    "    Fri = [\"NO\", \"NO\", \"NO\", \"YES\", \"YES\", \"NO\", \"NO\", \"NO\", \"YES\", \"YES\", \"NO\", \"YES\"]\n",
    "    # Hun = [] # excluded per problem statement\n",
    "    Pat = [\"Some\", \"Full\", \"Some\", \"Full\", \"Full\", \"Some\", \"None\", \"Some\", \"Full\", \"Full\", \"None\", \"Full\",]\n",
    "    Price = [\"$$$\",\"$\",\"$\",\"$\",\"$$$\",\"$$\",\"$\",\"$$\",\"$\",\"$$$\",\"$\",\"$\"]\n",
    "    Rain = [\"NO\", \"NO\", \"NO\", \"YES\", \"NO\", \"YES\", \"YES\", \"YES\", \"YES\", \"NO\", \"NO\", \"NO\"]\n",
    "    Res = [\"YES\", \"NO\", \"NO\", \"NO\", \"YES\", \"YES\", \"NO\", \"YES\", \"NO\", \"YES\", \"NO\", \"NO\"]\n",
    "    Type = [\"French\", \"Thai\", \"Burger\", \"Thai\", \"French\", \"Italian\", \"Burger\", \"Thai\", \"Burger\", \"Italian\", \"Thai\", \"Burger\"]\n",
    "    Est = [\"0-10\", \"30-60\", \"0-10\" , \"10-30\", \">60\", \"0-10\", \"0-10\", \"0-10\", \">60\", \"10-30\", \"0-10\", \"30-60\"]\n",
    "    # target\n",
    "    Target = [\"Y\",\"N\",\"Y\",\"Y\",\"N\",\"Y\",\"N\",\"Y\",\"N\",'N',\"N\",\"Y\"] #WillWait\n",
    "\n",
    "    Vars = [Alt, Bar, Fri, Pat, Price, Rain, Res, Type, Est, Target]\n",
    "\n",
    "\n",
    "    ### ENCODER DICTIONARY\n",
    "    encoder_dict = {\"NO\":0, \n",
    "                         \"YES\":1,\n",
    "                         \"N\":0, \n",
    "                         \"Y\":1,\n",
    "                         \"None\":0, \n",
    "                         \"Some\":1, \n",
    "                         \"Full\":2, \n",
    "                         \"$\":0, \n",
    "                         \"$$\":1, \n",
    "                         \"$$$\":2, \n",
    "                         \"French\":0, \n",
    "                         \"Thai\":1, \n",
    "                         \"Burger\":2, \n",
    "                         \"Italian\":3, \n",
    "                         \"0-10\":0, \n",
    "                         \"10-30\":1, \n",
    "                         \"30-60\":2, \n",
    "                         \">60\":3}\n",
    "    \n",
    "\n",
    "    ### TRANSFORM DATA [ENCODE]\n",
    "    transformed_vars = []\n",
    "    for Var in Vars:\n",
    "        transformed_vars.append([encoder_dict[x] for x in Var])\n",
    "\n",
    "    X = np.array([x for x in transformed_vars])\n",
    "    #print(X.shape) # features loaded in rows\n",
    "\n",
    "    df = pd.DataFrame(X.T) # cols are features\n",
    "    print(\"%s observations consisting of %s features, including target\" % (df.shape))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Feature Names\n",
    "\n",
    "feature_names = [\"Alt\", \"Bar\", \"Fri\", \"Pat\", \"Price\", \"Rain\", \"Res\", \"Type\", \"Est\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting on feature: 0, results in entropy: 1.0 and info gain: 0.0\n",
      "splitting on feature: 1, results in entropy: 1.0 and info gain: 0.0\n",
      "splitting on feature: 2, results in entropy: 0.9793 and info gain: 0.0207\n",
      "splitting on feature: 3, results in entropy: 0.4591 and info gain: 0.5409\n",
      "splitting on feature: 4, results in entropy: 0.8043 and info gain: 0.1957\n",
      "splitting on feature: 5, results in entropy: 0.9793 and info gain: 0.0207\n",
      "splitting on feature: 6, results in entropy: 0.9793 and info gain: 0.0207\n",
      "splitting on feature: 7, results in entropy: 1.0 and info gain: 0.0\n",
      "splitting on feature: 8, results in entropy: 0.7925 and info gain: 0.2075\n"
     ]
    }
   ],
   "source": [
    "### METHODS DEMONSTRATION: \n",
    "# entropy, information_gain, compute_info_purity from \"utils.DTL_methods\" package\n",
    "#\n",
    "# entropy –– used for computing information gain\n",
    "# information_gain –– computes information gain\n",
    "# compute_info_purity –– used to compute information purity from a given target vector\n",
    "\n",
    "# entropy & information gain\n",
    "num_features = df.shape[1]-1\n",
    "for i in range(num_features):\n",
    "    entropy = utils.DTL_methods.entropy(df, attribute=i, debug_flag=False)\n",
    "    info_gain = utils.DTL_methods.info_gain(dataset=df, attribute=i, debug_flag=False)\n",
    "    print('splitting on feature: %s, results in entropy: %s and info gain: %s' % (i,round(entropy,4), round(info_gain,4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split on attribute 3 \n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "attribute 3 = 0: \n",
      "sub-dataset\n",
      "    0  1  2  3  4  5  6  7  8  9\n",
      "6   0  1  0  0  0  1  0  2  0  0\n",
      "10  0  0  0  0  0  0  0  1  0  0\n",
      "attribute 3 = 1: \n",
      "sub-dataset\n",
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  1  0  0  1  2  0  1  0  0  1\n",
      "2  0  1  0  1  0  0  0  2  0  1\n",
      "5  0  1  0  1  1  1  1  3  0  1\n",
      "7  0  0  0  1  1  1  1  1  0  1\n",
      "attribute 3 = 2: \n",
      "sub-dataset\n",
      "    0  1  2  3  4  5  6  7  8  9\n",
      "1   1  0  0  2  0  0  0  1  2  0\n",
      "3   1  0  1  2  0  1  0  1  1  1\n",
      "4   1  0  1  2  2  0  1  0  3  0\n",
      "8   0  1  1  2  0  1  0  2  3  0\n",
      "9   1  1  1  2  2  0  1  3  1  0\n",
      "11  1  1  1  2  0  0  0  2  2  1\n"
     ]
    }
   ],
   "source": [
    "### DTL pipeline\n",
    "\n",
    "def split(dataset, cache=[]):\n",
    "    \"\"\"\n",
    "    dataset input, \n",
    "    computes the attribute to split on, adds attribute to cache,\n",
    "    outputs list of resulting sub-datasets \n",
    "    (with list label indicating what value the attribute attains on that sub-dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### compute splitting attribute which maximizes info gain\n",
    "    num_features = dataset.shape[1] - 1 # not counting target\n",
    "    gains = []\n",
    "    for i in range(num_features):\n",
    "        gains.append(utils.DTL_methods.info_gain(dataset,i))\n",
    "\n",
    "#     split_on = gains.index(max(gains)) # here we don't distinguish between if multiple features tie for best info gain\n",
    "\n",
    "    splitting_attributes = [i for i, j in enumerate(gains) if j == max(gains)]\n",
    "    split_on = splitting_attributes[0]\n",
    "    ### compute subdatasets\n",
    "    values = set(dataset[split_on])\n",
    "    results = []\n",
    "    \n",
    "    for x in values:\n",
    "        results.append([x,utils.DTL_methods.subset(dataset,split_on,x)])\n",
    "    \n",
    "    cache.append([[split_on,splitting_attributes,max(gains)], results])\n",
    "    \n",
    "    return results, cache\n",
    "                    \n",
    "\n",
    "# example\n",
    "results, cache = split(df, cache=[])\n",
    "print(\"split on attribute %s\" % cache[0][0][0], '\\n'+'%'*40)\n",
    "\n",
    "for pair in results:\n",
    "    print(\"attribute %s = %s: \\nsub-dataset\" % (cache[0][0][0],pair[0]))\n",
    "    print(pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §1 Growing Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We step through the process of growing a decision tree from the problem dataset, giving intermediate results and calculations at each step, and showing how the decision tree is split (computation of information gain is automated via the information gain utility method). Reading through, I hope it is fairly obvious how the entire process, including printing to console or otherwise of intermediate results, could be automated in an iterative fashion (including potentially a stopping / pruning heuristic to prevent tree overfitting).\n",
    "\n",
    "### step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on attribute 'Pat', with information gain 0.5408520829727552\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "\n",
      "Splitting on Pat = 'None' results in impurity of 0 \n",
      "Resulting Class Label: 0 (i.e., No)\n",
      "\n",
      "Splitting on Pat = 'Some' results in impurity of 0 \n",
      "Resulting Class Label: 1 (i.e., Yes)\n",
      "\n",
      "Splitting on Pat = 'Full' results in impurity of 0.9182958340544896 \n"
     ]
    }
   ],
   "source": [
    "current_iteration = 0\n",
    "\n",
    "results, cache = split(df, cache=[])\n",
    "\n",
    "splitting_info = cache[0][0]\n",
    "attrib_name = feature_names[splitting_info[0]]\n",
    "print(\"Split on attribute '%s', with information gain %s\" % (attrib_name, splitting_info[-1]))\n",
    "if len(splitting_info[1])>1:\n",
    "    print(\", \".join([feature_names[x] for x in splitting_info[1]])+\" tied for optimal information gain \\n(tie breaking was carried out arbitrarily)\")\n",
    "print('%'*50+'\\n'*2)\n",
    "\n",
    "\n",
    "pat_vals = [\"None\",\"Some\",\"Full\"]\n",
    "target_vals = [\"No\",\"Yes\"]\n",
    "\n",
    "for pair in results:\n",
    "    attrib_val = pair[0]\n",
    "    subdataset = pair[1]\n",
    "    target = subdataset.iloc[:,-1]\n",
    "    impurity = utils.DTL_methods.compute_info_purity(target)\n",
    "    print(\"Splitting on %s = '%s' results in impurity of %s \" % (attrib_name, pat_vals[attrib_val], impurity))\n",
    "    if impurity == 0:\n",
    "        label = list(target)[0]\n",
    "        print(\"Resulting Class Label: %s (i.e., %s)\" % (label, target_vals[label]), end='\\n'*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "Splitting along the feature Pat = 0,1 the impurity is now 0, i.e., the target lies entirely in one class on the resulting data subsets and hence those splits result in leaf nodes. For Pat = 2, we must continue to grow the DT from this node. So we arrive at an initial decision tree:\n",
    "![decision tree at step 1](trees/tree1.png \"tree 1\")\n",
    "### step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on attribute 'Price', with information gain 0.2516291673878229\n",
      "Price, Res, Type, Est tied for optimal information gain \n",
      "(tie breaking was carried out arbitrarily)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "\n",
      "Splitting on Price = '$' results in impurity of 1.0 \n",
      "Splitting on Price = '$$$' results in impurity of 0 \n",
      "Resulting Class Label: 0 (i.e., No)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_iteration = 1\n",
    "\n",
    "previous_results = cache[current_iteration-1][1]\n",
    "new_df = previous_results[2][1]\n",
    "\n",
    "## decided not to bother stripping out used attributes \n",
    "## this is code that would have adjusted namespace indexing if I did this\n",
    "# removed_feature = cache[-1][0][0]\n",
    "# remaining_features = [x for x in feature_names if x != remaining_features[removed_feature]]\n",
    "# remaining_features = feature_names\n",
    "\n",
    "results, cache = split(new_df, cache)\n",
    "\n",
    "splitting_info = cache[-1][0]\n",
    "attrib_name = feature_names[splitting_info[0]]\n",
    "print(\"Split on attribute '%s', with information gain %s\" % (attrib_name, splitting_info[-1]))\n",
    "\n",
    "if len(splitting_info[1])>1:\n",
    "    print(\", \".join([feature_names[x] for x in splitting_info[1]])+\" tied for optimal information gain \\n(tie breaking was carried out arbitrarily)\")\n",
    "print('%'*50+'\\n'*2)\n",
    "\n",
    "poss_att_vals = [\"$\",\"$$\",\"$$$\"]\n",
    "\n",
    "for pair in results:\n",
    "    attrib_val = pair[0]\n",
    "    subdataset = pair[1]\n",
    "    target = subdataset.iloc[:,-1]\n",
    "    impurity = utils.DTL_methods.compute_info_purity(target)\n",
    "    print(\"Splitting on %s = '%s' results in impurity of %s \" % (attrib_name, poss_att_vals[attrib_val], impurity))\n",
    "    if impurity == 0:\n",
    "        label = list(target)[0]\n",
    "        print(\"Resulting Class Label: %s (i.e., %s)\" % (label, target_vals[label]), end='\\n'*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "    Subsequently we select the feature Price to split along. This purifies a single leaf node, resulting in a class label of No. We then split along the remaining data (cheap restaurants, i.e. Price = $), we must continue to grow the DT from this node. We grow our decision tree:\n",
    "![decision tree at step 2](trees/tree2.png \"tree 2\")\n",
    "### step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on attribute 'Est', with information gain 0.5\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "\n",
      "Splitting on Est = '10-30' results in impurity of 0 \n",
      "Resulting Class Label: 1 (i.e., Yes)\n",
      "\n",
      "Splitting on Est = '30-60' results in impurity of 1.0 \n",
      "Splitting on Est = '>60' results in impurity of 0 \n",
      "Resulting Class Label: 0 (i.e., No)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_iteration = 2\n",
    "index_of_subdataset = 0\n",
    "previous_results = cache[current_iteration-1][1]\n",
    "new_df = previous_results[index_of_subdataset][1]\n",
    "\n",
    "results, cache = split(new_df, cache)\n",
    "\n",
    "splitting_info = cache[-1][0]\n",
    "attrib_name = feature_names[splitting_info[0]]\n",
    "print(\"Split on attribute '%s', with information gain %s\" % (attrib_name, splitting_info[-1]))\n",
    "\n",
    "if len(splitting_info[1])>1:\n",
    "    print(\", \".join([feature_names[x] for x in splitting_info[1]])+\" tied for optimal information gain \\n(tie breaking was carried out arbitrarily)\")\n",
    "print('%'*50+'\\n'*2)\n",
    "\n",
    "poss_att_vals = [\"0-10\",\"10-30\",\"30-60\",\">60\"]\n",
    "\n",
    "for pair in results:\n",
    "    attrib_val = pair[0]\n",
    "    subdataset = pair[1]\n",
    "    target = subdataset.iloc[:,-1]\n",
    "    impurity = utils.DTL_methods.compute_info_purity(target)\n",
    "    print(\"Splitting on %s = '%s' results in impurity of %s \" % (attrib_name, poss_att_vals[attrib_val], impurity))\n",
    "    if impurity == 0:\n",
    "        label = list(target)[0]\n",
    "        print(\"Resulting Class Label: %s (i.e., %s)\" % (label, target_vals[label]), end='\\n'*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "    Information gain next selects 'Est' (estimated wait time) to split along. This yields completely determined rules (leaf nodes) when Est = '10-30', '>60', but an impure sub-dataset for Est = '30-60' –– we continue to grow the DT from this node:\n",
    "![decision tree at step 3](trees/tree3.png \"tree 3\")\n",
    "### step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on attribute 'Bar', with information gain 1.0\n",
      "Bar, Fri, Type tied for optimal information gain \n",
      "(tie breaking was carried out arbitrarily)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "\n",
      "Splitting on Bar = 'No' results in impurity of 0 \n",
      "Resulting Class Label: 0 (i.e., No)\n",
      "\n",
      "Splitting on Bar = 'Yes' results in impurity of 0 \n",
      "Resulting Class Label: 1 (i.e., Yes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_iteration = 3\n",
    "index_of_subdataset = 1\n",
    "previous_results = cache[current_iteration-1][1]\n",
    "new_df = previous_results[index_of_subdataset][1]\n",
    "\n",
    "results, cache = split(new_df, cache)\n",
    "\n",
    "splitting_info = cache[-1][0]\n",
    "attrib_name = feature_names[splitting_info[0]]\n",
    "print(\"Split on attribute '%s', with information gain %s\" % (attrib_name, splitting_info[-1]))\n",
    "\n",
    "if len(splitting_info[1])>1:\n",
    "    print(\", \".join([feature_names[x] for x in splitting_info[1]])+\" tied for optimal information gain \\n(tie breaking was carried out arbitrarily)\")\n",
    "print('%'*50+'\\n'*2)\n",
    "\n",
    "poss_att_vals = [\"No\",\"Yes\"]\n",
    "\n",
    "for pair in results:\n",
    "    attrib_val = pair[0]\n",
    "    subdataset = pair[1]\n",
    "    target = subdataset.iloc[:,-1]\n",
    "    impurity = utils.DTL_methods.compute_info_purity(target)\n",
    "    print(\"Splitting on %s = '%s' results in impurity of %s \" % (attrib_name, poss_att_vals[attrib_val], impurity))\n",
    "    if impurity == 0:\n",
    "        label = list(target)[0]\n",
    "        print(\"Resulting Class Label: %s (i.e., %s)\" % (label, target_vals[label]), end='\\n'*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "    The decision tree has completely purified the data at this step, and we halt the learning process with a complete chain of rules:\n",
    "![decision tree at step 4](trees/tree4.png \"tree 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
